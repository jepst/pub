%-----------------------------------------------------------------------------
%
%               Template for sigplanconf LaTeX Class
%
% Name:         sigplanconf-template.tex
%
% Purpose:      A template for sigplanconf.cls, which is a LaTeX 2e class
%               file for SIGPLAN conference proceedings.
%
% Author:       Paul C. Anagnostopoulos
%               Windfall Software
%               978 371-2316
%               paul@windfall.com
%
% Created:      15 February 2005
%
%-----------------------------------------------------------------------------


\documentclass[preprint]{sigplanconf}


% The following \documentclass options may be useful:
%
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% authoryear    To obtain author/year citation style instead of numeric.

\usepackage{amsmath}
\usepackage{listings}
\usepackage{verbatim} % only for comment environment

\begin{document}

\lstset{language=Haskell, basicstyle=\footnotesize, numbers=none, numberstyle=\footnotesize, stepnumber=1, numbersep=5pt, showspaces=false, showtabs=false, frame=none, tabsize=2, captionpos=b, breaklines=true, breakatwhitespace=false, xleftmargin=10pt}

% from http://www.haskell.org/haskellwiki/Literate_programming
\lstnewenvironment{code}
    {\lstset{}%
      \csname lst@SetFirstLabel\endcsname}
    {\csname lst@SaveFirstLabel\endcsname}
    \lstset{
      basicstyle=\small\ttfamily,
      flexiblecolumns=false,
      basewidth={0.5em,0.45em},
      literate={+}{{$+$}}1 {/}{{$/$}}1 {*}{{$*$}}1 {=}{{$=$}}1
               {>}{{$>$}}1 {<}{{$<$}}1 {\\}{{$\lambda$}}1
               {\\\\}{{\char`\\\char`\\}}1
               {->}{{$\rightarrow$}}2 {>=}{{$\geq$}}2 {<-}{{$\leftarrow$}}2
               {<=}{{$\leq$}}2 {=>}{{$\Rightarrow$}}2 
%               {\ .}{{$\circ$}}2 {\ .\ }{{$\circ$}}2
               {>>}{{>>}}2 {>>=}{{>>=}}2
               {|}{{$\mid$}}1               
    }


\conferenceinfo{WXYZ '05}{date, City.} 
\copyrightyear{2011} 
\copyrightdata{[to be supplied]} 

\titlebanner{banner above paper title}        % These are ignored unless
\preprintfooter{short description of paper}   % 'preprint' option specified.





%%%%% Andrew's commands
\newcommand{\ct}[1]{\textsf{#1}}

\newcommand{\nfn}{\ensuremath{\stackrel{\hash}{\rightarrow}}}
\newcommand{\nlambda}{\ensuremath{\lambda^\hash}\,}
\newcommand{\napp}{\ensuremath{\;\ct{\small \$}^{\hash}\,}}

\newcommand{\Int}{\mathbb{Z}}
\newcommand{\pair}[2]{\mbox{$\langle$#1, #2$\rangle$}}
\newcommand{\mpair}[2]{\mbox{$\langle #1, #2 \rangle$}}
\newcommand{\entails}{\vdash}
\newcommand{\hash}{\texttt{\#}}
\newcommand{\closed}[1]{\hash\,#1}
\newcommand{\textt}[1]{\lstinline!#1!}





\title{Haskell for the cloud}
\subtitle{}

\authorinfo{Jeff Epstein}
           {University of Cambridge}
           {jee36@cam.ac.uk}
\authorinfo{Andrew P. Black}
           {Portland State University\titlenote{Currently on sabbatical at Microsoft Research, Cambridge.}}
           {black@cs.pdx.edu}
\authorinfo{Simon Peyton-Jones}
           {Microsoft Research, Cambridge}
           {simonpj@microsoft.com}

\maketitle

\begin{abstract}
% 1. What's the problem?
% 2. What does it matter?
% 3. What's the solution?
% 4. What are the consequences?
We present a framework for developing Haskell programs to be run in a distributed computing environment. It provides a message-passing communication model, inspired by Erlang, without introducing incompatibility with Haskell's established shared-memory concurrency. We believe our framework will let Haskell programmers create fault-tolerant, high-performance distributed systems with a minimum of effort, without giving up Haskell's strengths in strong typing and traditional concurrent programming.

\end{abstract}

% \category{CR-number}{subcategory}{third-level}
\category{D.1.3}{Programming Techniques}{Concurrent Programming---Distributed Programming}
\category{D.3.2}{Programming Languages}{Haskell}
%\category{D.3.2}{Programming Languages}{Applicative (functional) languages}

\terms
Languages, Reliability, Performance

\keywords
keyword1, keyword2

\section{Introduction}

With the age of steadily improving processor performance behind us, the way forward seems to be to compute with more, rather than faster, processors. A data center for storing and processing end users' data or running users' programs on a  group of servers is termed a {\em cloud}. We'll use this term to mean specifically a network of connected computers, having independent failure modes and separate memories. But how to program such a system?

Developing for the cloud presents some unique challenges. There is the obvious task of coordinating program control between many, possibly heterogeneous computers. In addition, the programmer needs to expect that in a network of dozens or hundreds of computers, some of them will fail, and so any such cloud system needs to be able to tolerate partial failure of any of its constituents. And, of course, the cloud needs to be able to deliver high performance, particularly as it relates to data locality: without shared memory, a major drain on performance is the transmission of data across the network, and so the framework needs to expose some ability to control the application's {\em cost model}, that is, which data is moved and when.

How shall we address these issues? To the extent that modern, popular programming languages support concurrency, it's usually of a shared-memory variety: that is, it presents the programmer with the abstraction of multiple concurrent threads accessing a pool of mutable common data. This model is clearly inappropriate for programming concurrency in a cloud, as there is in fact no shared memory between physically independent computers.

The solution we use, popularized by MPI \cite{mpi99} and Erlang \cite{Erlang93}, is {\em message passing}. The message passing model stipulates that the concurrent threads have no implicit access to each other's data, but instead share data explicitly by sending and receiving {\em messages}. 

Even better, the message-passing model for thread communication eliminates some of the classic concurrency pitfalls, such as race conditions, which cause bugs whose detection and removal may be beyond the skill of many programmers. 

Haskell is an ideal language for this problem space. As a pure functional language, data is by default immutable, so the lack of shared mutable data won't be missed. Also, since code is by default idempotent, code running on failing hardware can be restarted elsewhere without the need for distributed transactions.

The contributions of this paper are:

\begin{itemize}
\item A proposal for a distributed programming API for Haskell (Section 2). Following the Erlang model, our framework provides a system for exchanging messages between concurrent threads, regardless of whether those threads are running on one computer or on many. Besides mechanisms for sending and receiving data, the API provides functions for starting new threads remotely. Our framework also offers fault tolerance, again closely following the widely-respected Erlang model. Unlike Erlang, our framework does not prohibit the use of explicit shared memory concurrency mechanisms.
\item A method for serializing function closures to enable higher-order functions to work in a distributed environment (Section 3). The need to start threads remotely demands a representation of code objects and their environment. Our approach to closures requires explicit indication of which parts of the function's environment will be serialized and thus gives the programmer greater control over his or her application's cost model.
\item A demonstration of the effectiveness of our approach in the form of example applications (Section 4). We provide performance measurements from the k-means clustering algorithm, an iterative algorithm for partitioning data point into natural groups. The highly parallel nature of this algorithm makes it well-suited for deployment in a distributed environment.
\end{itemize}

\section{Processes and messages}

\subsection{Processes}
The basic unit of concurrency in our framework is the {\em process}. A process is a thread that has been ``blessed'' with the ability to send and receive messages. Processes are lightweight, having low overhead to create and run, because they are based on Haskell's \textt{forkIO} lightweight threads, and thus unlike heavyweight OS threads, the programmer can create larger numbers of concurrent processes.

Processes are identified by a unique process ID. When creating a new process, the its ID will be returned, and can be used to send messages to the new process.

In most respects, our framework follows the Erlang model by favoring message-passing as the primary means of communication between processes. Our framework differs from Erlang, though, in that it does not outright prohibit shared-memory concurrency. The existing elements of Haskell's concurrency model, such as \textt{MVar} for shared mutable variables and \textt{forkIO} for creating lightweight threads, are still available to programmers who wish to combine message passing with the more traditional approach. The framework ensures that mechanisms specific to shared memory concurrency cannot be inadvertently used between remote systems. 

\subsection{Messages to processes}

Any process can send and receive messages. Our messages are asynchronous, reliable, and buffered. This functionality is implemented in the \textt{ProcessM} monad, such that all the state associated with messaging (most especially, the message queue) is wrapped in that data structure, which is updated with each statement. Thus, any code participating in messaging must be in the \textt{ProcessM} monad.

Consider a simple process that accepts ``pong'' messages and responds by sending a corresponding ``ping'' to whatever process sent the pong. Using our framework, the code for such a process would look like this:

\begin{code}[caption={Ping in Haskell}]
data Ping = Ping ProcessId
data Pong = Pong ProcessId
-- omitted: Serializable instance for Ping and Pong

ping :: ProcessM ()
ping self = 
   do { self <- getSelfPid
        receiveWait [
          match (\ (Pong partner) -> 
            send partner (Ping self)) ]
      ; ping self }
\end{code}

The equivalent code in Erlang looks like this:

\begin{code}[language=Erlang,caption={Ping in Erlang}]
ping() ->
  receive
    {pong, Partner} -> 
      Partner ! {ping, self()}
  end,
  ping().               
\end{code}

These two programs have nearly identical structure. Both \textt{ping} functions are designed to be run as processes. They each wait for a specific message to be received; the Haskell program matches incoming messages by type, whereas in Erlang, messages are usually pattern-matched against tuples. The programs look for a ``pong'' message, and ignore all others. The ``pong'' message contains in its payload a process ID, to which the response message is sent, this time containing the process ID of the \textt{ping} process (given in both languages as \textt{self}). Finally, they wait for the next message by repeating with tail recursion.

If this example looks familiar, it should: it's very close to the first distributed programming example given in {\em Getting Started with Erlang}. Note that in the Haskell version, unlike in the Erlang version, \textt{Ping} and \textt{Pong} are types rather than atoms, and so they need to be declared explicitly. As given, the type declarations are incomplete, as the instance declarations have been omitted for brevity.

Here we present some of the important functions that form the messaging API of our framework.

\begin{itemize}
\item 
\begin{code}
send :: (Serializable a) => ProcessId -> a -> ProcessM ()
\end{code}

To send a message, use \textt{send}, which packages up an arbitrary chunk of data and transmits it (possibly over the network) to a particular process, given by its unique \textt{ProcessID}. Upon receipt, the incoming message will be placed in a message queue associated with the destination process. The data to be transmitted must implement the \textt{Serializable} type class, as well as Haskell's \textt{Data.Binary.Binary} class, which allows the data to be converted to a form suitable for transmission. The \textt{send} function corresponds to Erlang's \texttt{!} operator.

While all of Haskell's primitive data types and most of the common higher-level data structures are instances of \textt{Serializable}, and therefore can be part of a message, some data types are emphatically not serializable. One example of this is \textt{MVar}, Haskell's type for mutable concurrent variables. Since \textt{MVar} allows communication between threads on the assumption of shared memory, it isn't helpful to send it to a process that has no shared memory with the current process. Although one can imagine a synchronous distributed variable that mimics the semantics of an \textt{MVar}, such a variable would have a vastly different cost model than \textt{MVar}. Since neither \textt{MVar}'s cost model nor its implementation could be preserved in an environment requiring communication between remote systems, we felt it best to prohibit programmers from trying to use it in that way. Nevertheless, \textt{MVar}s can be used within a single process: processes are allowed to use Haskell's \textt{forkIO} function to create local threads that can share memory using \textt{MVar}.

\item 
\begin{code}
receiveWait :: [MatchM q ()] -> ProcessM q
match :: (Serializable a) => (a -> ProcessM q) -> MatchM q ()
\end{code}

On the receiving end, \textt{receiveWait} and \textt{match}, which operate as a pair, examine the message queue associated with the current process and extract a received message, unpacking the transmitted chunk of data and executing an associated block of user code. Together, they correspond to Erlang's \textt{receive} construct. Since our framework is packaged as a library rather than as a language extension, we use the \textt{MatchM} type to approximate Erlang's specialized syntax. \textt{receiveWait}'s first parameter is a list of \textt{match} invocations, where the lambda function argument to each \textt{match} potentially accepts a different type of message. Thus, the programmer can selectively dequeue messages of particular types. As in Erlang, incoming messages are tested in the order that the matching patterns appear. If no message in the queue is of any of the acceptable types, \textt{receiveWait} will block until such a message is received. % maybe mention matchIf, receiveTimeout, etc

In the ping example above, we use \textt{receiveWait} and \textt{match} to accept messages only of type \textt{Pong}. The type of message to accept is specified through Haskell's type inference: the lambda function given as the first parameter to \textt{match} has type \lstinline!Pong -> ProcessM ()!, and so that invocation of \textt{match} will accept messages only of type \textt{Pong}.
\end{itemize}

\subsection{Messages through channels}
In the previous subsection, we've shown how a message can be sent to a process. As you can see from the type signature of \textt{send}, essentially any serializable data structure can be sent as a message to any process. Whether or not a particular message will be accepted (i.e. dequeued and acted upon) by the recipient process isn't determined until runtime. But what about Haskell's strong typing? Wouldn't it be nice to have some static guarantees that we are sending a message type to a receiver that knows how to deal with it?

Thus, an alternative to sending messages by process identifier is to use {\em typed channels}. Each distributed channel consists of two ends, which we call the {\em send port} and {\em receive port}. Messages are inserted via the send port, and extracted in FIFO order from the receive port. Unlike process identifiers, channels are associated with a particular type and the send port will emit messages only of that type; likewise, the receive port will accept messages only of that type, so the sender has a guarantee that its receiver is of the right type.

The central functions of the channel API are:

\begin{code}
newChannel :: (Serializable a) => ProcessM (SendPort a, ReceivePort a)
sendChannel :: (Serializable a) => SendPort a -> a -> ProcessM ()
receiveChannel :: (Serializable a) => ReceivePort a -> ProcessM a
\end{code}

A critical point is that although \textt{SendPort} can be serialized and copied to other nodes, allowing the channel to accept data from multiple sources, the \textt{ReceivePort} cannot be moved from the node on which it was created. We decided that allowing a movable and copyable message destination would introduce too much complexity. This restriction is enforced by making \textt{SendPort} an instance of \textt{Serializable}, but not \textt{ReceivePort}. 

We can now reformulate our ping example to use typed channels. The process must be given two ports: a receive port on which to receive pongs, and a send port on which to emit pings. Each \textt{Ping} and \textt{Pong} message now contains the send port on which its recipient should respond; thus the \textt{Ping} message contains the send port of a channel of pongs, and the \textt{Pong} message contains the send port of a channel of pings.

% this might not be the best example, really
\begin{code}
ping2 :: SendPort Ping -> ReceivePort Pong -> ProcessM ()
ping2 pingout pongin = 
   do { (Pong partner) <- receiveChannel pongin
      ; sendChannel partner (Ping pongin) 
      ; ping pingout pingin }
\end{code}

How do we start the exchange? Clearly we need to create two channels and call \textt{ping2} and \textt{pong2} (not shown, but substantially similar to \textt{ping2}) as new processes. But how do we start a new process?

\subsection{Starting processes}

To start a new process in a distributed system, we need a way of specifying where a process will run. The question of {\em where} is answered with our framework's unit of location, the node. A node can be thought of as an independent address space. Each node has a unique identifier, which contains the address and port number of a computer in the network. So, to be able to start processes, we want a function named \textt{spawn} that takes two parameters: a node identifier, saying on which computer the new process should run; and some expression of what code should be run there. Since we want to run code that is able to receive messages, the code should be in the \textt{ProcessM} monad. The function should then return a process identifier, which can be used with \textt{send}. And since the \textt{spawn} function itself depends on messaging, it, too, will be in the \textt{ProcessM} monad. As a first draft, let's consider this (incorrect) possibility:

\begin{code}
-- wrong
spawn :: NodeId -> ProcessM () -> ProcessM ProcessId
\end{code}

In combination with the \textt{ping} and \textt{pong} functions, it could be used like this:

\begin{code}
-- wrong
do { pingProc <- spawn someNode ping
   ; pongProc <- spawn otherNode pong
   ; send pingProc (Pong pongProc) }
\end{code}

The above example is supposed to start two new processes on \textt{someNode} and \textt{otherNode}, with each process expecting to receive messages of a particular type. To begin the exchange, we send an initial message to the ping process.

To understand why this version of \textt{spawn} is wrong, consider how to implement it. Assuming we have the ability to send messages containing arbitrary serializable data, \textt{spawn} can be implemented using \textt{send}.  \textt{spawn} will send a message containing its second parameter to a ``spawning'' process on the remote node; that is, a process that starts new processes in response to messages received from \textt{spawn}. In the above case, the call to \textt{spawn} would send a message containing the function \textt{ping}. And there's the sticky wicket. We can easily imagine how to serialize a string, or a list, or an algebraic data type composed of primitive types, but \textt{ping} is none of these. It's a function. And what does it mean to serialize a function? The question is especially important in a language like Haskell, where so much depends on higher-order functions manipulating other functions as data.

The other problem with serializing functions is that it isn't enough to serialize just the function: we also need its environment, precisely, its free names. Some of the free names used by the \textt{ping} function, such as \textt{receiveWait}, are top-level. Assuming that the same code is running on all hosts (a nontrivial assumption), it's not necessary to transmit the value of top-level names, as we know that they already exist at the destination node. But consider how to serialize a function that has free names that are not top-level, such as this one:

\begin{code}
-- wrong
printSumSomewhere :: NodeId -> Int -> ProcessM ()
printSumSomewhere aNode i =
  let nums = [0..i]
      fun = liftIO (putStrLn (show (sum nums)))
   in spawn aNode fun
\end{code}

This function accepts a location, given as a \textt{NodeId}, and an integer \textt{i}, and should remotely run the function \textt{fun}, which calculates the sum of integers $0 \ldots i$, and then prints out the result at the remote node. The list of integers $0 \ldots i$, named here \textt{nums}, is a free variable in \textt{fun}, but is not top-level. Furthermore, \textt{nums} depends on \textt{i}, which is also not top-level. Therefore, to be able to run \textt{fun} on a remote node, the local value of \textt{nums}, or at least \textt{i}, would need to be serialized and transmitted along with the representation of \textt{fun}.

There are a few reasons why we don't want to automatically transmit the whole of \textt{fun}'s environment. The first reason is that it's hard to do without extending the language. In order to discover what names need to be included as part of a serializable environment, we would need to traverse \textt{fun}'s abstract syntax tree, picking up free variables along the way, and in turn the transitive closures of their free variables, as well. Since our framework is implemented solely as a library, we'd prefer to avoid the compiler-hacking that would be necessary.

Another reason why implicit serialization of environment is bad is that it's easy for the programmer to lose track of what's being serialized. Since over-the-wire communication is potentially the greatest bottleneck in a distributed application, it's important that the programmer have direct control over what is transmitted. In the above code, for example, \textt{i} may be serialized, even though it isn't mentioned in \textt{fun}. To keep the quantity of serialized data under control, we prefer an explicit approach to environment serialization.

Finally, we should mention that in a non-strict language such as Haskell, implicit serialization of environment raises some thorny questions about what is evaluated and where. How, exactly, should \textt{fun}'s environment be serialized? One way would be for the value of \textt{nums} to be sent. Another way would be for the value of \textt{i} to be sent, along with a representation of \textt{nums} that depends on the value of \textt{i}. In the first option, depending on the size of \textt{nums}, a lot of data could potentially be transmitted. In the second option, there would probably be fewer bytes sent, at the cost of having to evaluate \textt{nums} remotely. There are several possible ways a language could balance these trade-offs, but the answer is likely to be somewhat arbitrary and not transparent to the programmer. This is another case where the programmer loses control of the cost model, and, again, we aimed for a more explicit interface.

In our framework, a captured environment is never transmitted implicitly. We accomplish this by restricting the set of serializable functions to those without non-top-level free variables. This is discussed further in section 3.

\subsection{Fault tolerance}
We aimed to follow as closely as possible Erlang's model for fault tolerance. Erlang's \textt{link} and \textt{monitor} functions allow processes to request notification if another process terminates. We provide analogous functions that offer similar functionality.

Our fault tolerance API is based on the idea that a process can monitor another process. If the monitored process terminates, the other process will be notified, and can take appropriate action. We provide relatively only low-level functions that allow for notification of possible faults. Actually recovering from faults is left to the application or a higher-level framework, such as the one briefly discussed in the Future work section.

A process can request to be notified in the event that another process terminates for any reason, or only in the case of a fault involving that process. A fault that will result in a notification can be caused either by the other process being terminated as a result of an uncaught exception, or if the node on which the process is running becomes inaccessible. The monitoring process can also indicate to be notified in one of two ways: either by an asynchronous exception, which can be caught with Haskell's usual exception handling mechanisms; or by message, which can be received using the \textt{receiveWait} function.

Here are the functions for setting up process monitoring:

\begin{code}
monitorProcess :: ProcessId -> ProcessId -> MonitorAction -> ProcessM ()
linkProcess :: ProcessId -> ProcessM ()
\end{code}

\lstinline!monitorProcess a b ma! establishes unidirectional process monitoring. That is, process \textt{a} will be notified if process \textt{b} terminates. The third argument determines whether the monitoring process will be notified by exception or by message.

\textt{linkProcess} corresponds to Erlang's \textt{link}. It establishes bidirectional process monitoring between the current process and a given process; if either of those processes terminate abnormally, the other will receive an asynchronous exception. \textt{linkProcess} is defined in terms of \textt{monitorProcess}. 





\section{Closures}
\begin{comment}
there's no way for a library to know what to transmit, or even if it's serializable. Clearly, we don't want to prevent remotely-invocable functions from ever using non-serializable data. That would mean that any function that could be started using \textt{spawn} wouldn't be allowed to use \textt{MVar}.

all parameters to closure must be serializable

I agree that all we need to do to represent deferred evaluation is a
<fun,arg> tuple, and that's basically what a Closure is. We'll still
serialize arbitrarily-sized environments, as long as they are
converted explicitly to an argument, and that explicit control is an
advantage. And, yes, the practical argument that implicitly
serializing environments would be messy to implement is also valid.

only local and global names are visible in a top-level function. This is a reasonable approximation because all toplevel names are always available on all hosts and never need to be transmitted

prevention of sharing bad things (e.g. mvars, receiveports) is enforced by the serializer
\end{comment}
% ----------------------------------------------- BEGIN SIMON

Any distributed implementation of a statically-typed, 
higher-order programming language must
grapple with the fundamental question of \emph{how to transmit closures} from
one node to another.  For example, consider:
\begin{code}
  sf :: SendPort (Int -> Int) -> Int -> ProcessM ()
  sf ch x = send ch (\y -> x+y)
\end{code}
A function value is a closure that captures its free variables,
\textt{x} in the example, so to serialise a function value
one must serialise its free variables.  But the types of
these free variables are unrelated to the type of the function
value, so it is entirely unclear \emph{how} to serialise them.
In concrete terms, there is no way to write this instance
declaration
\begin{code}
  instance ???? => Serialisable (a->b) where ????
\end{code}
One ``solution'' would be to say that functions are simply not
serialisable --- but exactly the same issue arises with \textt{spawn}
which is clearly essential.  Consider
\begin{code}
  nc :: ProcessM ()
  nc = do { (s,r) <- newChan
          ; spawn node (do { v <- receive r
                           ; ... })
          ; ... }
\end{code}
The \textt{ProcessM} argument of \textt{spawn} is a value closed over
its free variables, \textt{r} in this case, so exactly the same issue
arises.

\subsection{The standard solution}

The standard approach to this problem is to build in serialisation of
function values --- and indeed of all values --- as a primitive operation
implemented directly by the runtime system.  That is, the runtime system
allows one to serialise \emph{any value at all}, and transport it to the 
other end of the wire.  Now function closures can be serialised by
serialising their free variables, and adding a representation of their code.
This approach is used by every higher-order distributed-memory 
system that we know of, including GDH \cite{GDH}, JoCaml (check!!!), \emph{what else}.

But making serialisability built-in
has multiple disadvantages:
\begin{itemize}
\item It relies on a single built-in notion of serialisability.
In contrast, the \texttt{Serialisable} type class introduced in 
Section~\ref{s:serialisable} gives the programmer control over how
values are serialised.  For example, a data structure might have
redundant information cached in the nodes, which should be reconstructed
at the far end rather than being serialised.  
This is exactly what type
classes are for!
\item It is crucial that some types are \emph{not} serialisable. For
example, we do not want the receive port of a channels to be serialisable, 
so that senders know where to send their messages to.  Similarly making 
\textt{TVar}s non-serialisable guarantees that \textt{STM} transactions 
do not span processes.
\item Serialising a value and sending it over the network has an important
effect on the cost model; it should not be silent.
\end{itemize}
In short, built-in serialisability of every value is
too big a primitive.  We do need \emph{some} built-in support, but
we seek something more more modest.  Proposing such a mechanism is one of 
the main contributions of this paper.

\subsection{Static values}

We begin with a simple intuituion: some closures can readily be
transmitted to the other end of the wire, namely ones that have no
free variables.  For the present we make a simplifying assumption,
that every node is running the same code.  (We return to the question
of code that varies between nodes in Section~\ref{s:further-work}.)
Under this assumption, a closure without free variables can be
readily be serialised to a single label, or even (in the limit) a machine
address.  Types classify values, so it is natural to suggest 
a type $(\text{\tt Static}~\tau)$ to classify such values.  The type 
constructor \textt{Static} is a new built-in primitive, 
enjoying a built-in serialisation instance:
\begin{code}
  instance Serialisable (Static a)
\end{code}
It is helpful to remember this intuition: \emph{the defining property of
a value of type $(\text{\tt Static}\;\tau)$ is that it can be serialised},
can moreover can be serialised without knowledge of how to serialise $\tau$.
Operationally, the implementation serialises a \textt{Static} value by first evaluating it,
and then serialising the code label in the value thus obtained.  

\begin{figure}
\begin{minipage}{\linewidth}
$$
\begin{array}{rcl}
\Gamma & ::= & \overline{x :_{\delta} \sigma} \\
     \delta & ::= & S | D
\end{array}
$$
\begin{align*}
\Gamma\downarrow = \{ x:_{s} \sigma \mid x :_{s} \sigma \in \Gamma \}
\end{align*}

\begin{equation*}
\tag{Static~intro.}
\frac{\Gamma\downarrow~\entails e : \tau}
     {\Gamma \entails \text{\tt static}~e : \text{\tt Static}~\tau}
\end{equation*}

\begin{equation*}
\tag{Static~elim.}
\frac{\Gamma \entails e : \text{\tt Static}~\tau}
     {\Gamma \entails \text{\tt unstatic}~e : \tau}
\end{equation*}
\end{minipage}
\caption{Typing rules for \textt{Static}} \label{fig:static}
\end{figure}
Along with the new type constructor we introduce a new term,
$(\text{\tt static}\;e)$ to introduce it, and another 
$(\text{\tt unstatic}\;e)$ to elimiate it.
The simple typing judgements for these terms are given in Figure~\ref{fig:static}.
They embody the following key ideas:
\begin{itemize}
\item A \emph{variable is static} iff it is bound at the top level.
\item A \emph{term $(\text{\tt static}\;e)$ has type $(\text{\tt Static}\;\tau)$} iff 
(a) $e$ has type $\tau$, and (b) $e$ is static.  
\end{itemize}
The type environment $\Gamma$ is a set of variable bindings, each of form $x :_{\delta} \sigma$.
The subscript $\delta$ is a static-ness flag, which takes the values \textt{S} (static) or
\textt{D} (dynamic).  The idea is that top-level (static) variables have bindings
of form $f :_{\text{\tt S}}$, while all other variables have dynamic bindings $x :_{\text{\tt D}}$.
(It is straightforward to formalise this idea in the typing judgements for top-level
bindings and for terms; we omit the details.)
The operation $\Gamma \downarrow$ filters $\Gamma$ to leave only the static bindings,
thereby checking that a term $\text{\tt static}\;e$ is only well typed if its free
variables are all static.

Although simple, these rules have intersting consequences:
\begin{itemize}
\item A static variable may have a non-\textt{Static} type. Consider the 
top-level binding for the identity function:
\begin{code}
  id :: a -> a
  id x = x
\end{code}
The function \textt{id} is by definition static (top-level).  Its binding
in $\Gamma$ will have $\delta=\text{\tt S}$, but its type is the ordinary
polymoprhic type.

\item A non-static variable may have a \textt{Static} type.  For example
\begin{code}
  f :: Static a -> (Static a, Int)
  f x = (x, 3)
\end{code}
Here \texttt{x} is clearly not a static variable, but it certainly has
a \textt{Static} type.  So fully-dynamic functions
can readily compute over values of \textt{Static} type.

\item The free variables of a term $(\text{\tt static}\;e)$ need not have
type $(\text{\tt Static}\;\tau)$. For example, this term is well-typed:
\begin{code}
  static (length . filter id) :: Static ([Bool] -> Int)
\end{code}
because all its free variables (\textt{length}, \textt{(.)},
\textt{filter}, \textt{id}) are bound at top-level and hence are
static. However, all these functions have their usual types.
\end{itemize}

\subsection{From static values to closures}

In the examples \textt{sf} and
\textt{nc} (at the start of this Section) we wanted to transmit closure
that certainly did have free variables.  How do static values help us?
Answer: they help us by making closure conversion possible. A closure
is just a pair of a code pointer and an environment.  With the aid of
\textt{Static} values we can now represent a closure directly in Haskell:
\begin{code}
data Closure a where   -- Wrong
  MkClo :: Static (env -> a) 
        -> env -> Closure a
\end{code}
As is conventional, we capture the environment in an existential.
The trouble is that this closure type is not serialisable: precisely
because the environment is existentially quantified, there is no information
for how to serialise it!  This is apparently esay to solve, by asking
that the environment be serialisable:
\begin{code}
data Closure a where   -- Still wrong
  MkClo :: Serialisable env
        => Static (env -> a) -> env -> Closure a
\end{code}
Now serialisation is easy:
\begin{code}
instance Serialisable (Closure a) where
   encode (MkClo f env) = encode f ++ encode env
\end{code}
But how can we \emph{de}-serialise a closure?  The difficulty is
that, at the receiving end, we do not know the type captured inside
the closure, so we do not know what deserialiser to use.  This initially
appears to be a very awkward problem indeed. Maybe we have to send a 
representation of the environment type, and do a run-time type-class lookup
at the receiving end?  This solution is used by Clean \cite{clean}.  Maybe
we could send some representation of the deserialisation function itself?
But that seems to require a solution to the problem of serialising 
closures, so an infinite regress beckons.

Happily, the solution is simple and, with the benefit of hindsight,
obvious: perform both serialisation and deserialisation at \emph{closure-construction time},
not at \emph{closure-serialisation time}:
\begin{code}
data Closure a where   -- Right
  MkClo :: Serialisable env
        => Static (ByteString -> a) 
        -> ByteString -> Closure a
\end{code}
To see this in action, here is our earlier \textt{sf} example, 
expressed using closures:
\begin{code}
  sf :: SendPort (Closure (Int -> Int)) 
     -> Int -> ProcessM ()
  sf ch x = send ch clo
    where
      clo  = MkClo (static sfun) (encode x)

  sfun :: Static (ByteString -> Int -> Int)
  sfun = static (\bs -> let x = decode bs 
                        in \y -> x + y)
\end{code}
The closure contains the pre-serialised environment \textt{encode x},
and the static function \textt{sfun}. The latter de-serialises its
argument \textt{bs} to get the real argument \textt{x} that it expects.

It is easy to un-closure-convert:
\begin{code}
  unClo :: Closure a -> a
  unClo (MkClo f x) = unstatic f x
\end{code}
This is when the deserialisation of the environment takes place. For a
function-valued closure it makes sense to apply \textt{unClo} once, and
apply the resulting function many times, so that the deserialisation is
one just once.

Performing manual closure conversion is tiresome for the programmer,
and we describe some Template Haskell support in
Section~\ref{sect:th}.  But it has the great merit that it makes
crystal clear exactly what is serialised, and when.  

% ----------------------------------------------- BEGIN ANDREW

To provide explicit control over which data accompanies a remote function invocation, we use {\em closures}. For our purposes, a closure is a data structure that encapsulates both a representation of a function and its environment. Including the environment is especially important in a language like Haskell, where so much of the power of higher-order functions rests in their ability to accept functions that do capture local names, but we also want to prevent the programmer from accidentally transmitting an unnecessarily large environment.

What restrictions should we apply and how can we enforce them?

To illustrate our quest for a mechanism for remote function invocation, we introduce the function \textt{callPure}, a variation of \textt{spawn} that has this type:

\subsection{Closures in theory}

As we discussed in section 2.4, serializing functions that have free variables is problematic. We want to avoid the issue of automatically serializing potentially large chunks of the function's environment, possibly without the programmer's awareness. We can't just restrict serializable functions to those that have no free names, though, as even the most innocent function is likely to have at least one. Consider this straightforward function:

\begin{code}
greet :: String -> ProcessM ()
greet name = liftIO (putStrLn ("Hi, " ++ name))
\end{code}

In \textt{greet}, \textt{liftIO}, \textt{putStrLn} and \lstinline!(++)! are free names. Nevertheless, we feel that we should be able to run this function on a remote system using our (wrong) first attempt at \textt{spawn}:

\begin{code}
-- wrong
spawn someNode (greet "Jaroslav")
\end{code}

We'd also like to be able to run remotely functions that don't have side effects. Consider \textt{call}, a variation of \textt{spawn} that takes a location and an unevaluated expression, computes its value remotely, and returns it.

\begin{code}
-- also wrong
call :: (Serializable a) => NodeId -> a -> ProcessM a
\end{code}

Ideally, we'd like to use \textt{call} like this, even though \textt{add} captures the free name \lstinline!(+)!:

\begin{code}
add :: Int -> Int -> Int
add a b = a + b

-- wrong
res <- call someNode (add 3 4)
\end{code}

Clearly, the given signature for \textt{call} is wrong, since Haskell's lazy evaluation doesn't let us syntactically distinguish between  unevaluated and evaluated values, so the programmer doesn't have control over which computer the given expression would be computed on. What we want is a representation of a call to \textt{add} function that is distinct from actually calling it.

In order to construct a representation of functions, we posit a new attribute of names, orthogonal to their type. A name is {\em staticable} if either it is a top-level name or all its free variables are staticable. We denote the context of staticable names as:

\begin{align*}
\Gamma\downarrow = \{ x:_{s} \sigma \mid x :_{s} \sigma \in \Gamma \}
\end{align*}

To use the names denoted as staticable, we furthermore posit a special \lstinline!Static a! type that can wrap only staticable types. It has a type constructor, \lstinline!Static!, and the wrapped value can be extracted using the \textt{unStatic} function:

\begin{equation*}
\tag{Static~intro.}
\frac{\Gamma\downarrow~\entails e :_{s} \tau}
     {\Gamma \entails \text{Static}~e : \text{Static}~\tau}
\end{equation*}


\begin{equation*}
\tag{Static~elim.}
\frac{\Gamma \entails e : \text{Static}~\tau}
     {\Gamma \entails \text{unStatic}~e : \tau}
\end{equation*}

Now let us turn to the question of how to represent a function's environment. The names used in a function will fall into one of these categories:

\begin{enumerate}
\item Top-level names. We don't need to transmit these values, because we know that they exist on the remote system.
\item Parameter names. These values must be provided by the caller, and therefore must be transmitted to the remote system.
\item Local, non-top-level names. These values need to be present on the remote system, but for the reasons explained in section 2.4, we don't want to let the framework implicitly transmit them. If the programmer wants to explicitly include elements of a local environment in the remote function invocation, he or she can use the technique of {\em lambda lifting} \cite{lambdalifting} to convert the local names into parameters. % explain this here
\item Names defined within the function. These does need to be transmitted, as their values will be constructed by the function itself.
\end{enumerate}

We have reduced the problem of serializing the function's environment into the problem of serializing the function's parameters. So implementing functions like \textt{spawn} and \textt{call} should be a matter of packaing up the \textt{Static} function representation and its arguments, serializing them, sending them over the wire, deserializing the function representation and arguments, \textt{unStatic}-ing the function, and applying the arguments. We can imagine a {\em closure} data type that encapsulates both the function representation and an encoding of its environment:

\begin{code}
-- wrong
data Closure a = Closure (Static (env -> a)) env
\end{code}

A \textt{Closure a} packages up computation that will eventually compute a value of type \textt{b}, where \textt{b} is the return type of the function to invoke. That function's free variables have been turned into parameters, so we have to pass those parameters along as part of the closure, encoded as a type \textt{env}. Naturally, the parameters themselves must also be serializable. Note that the type of the parameters must match the input to the function. Concretely, we can imagine that \textt{env} is a tuple of the function's arguments. For example, a closure for the \textt{add} function might be written as \textt{Closure (Static add) (3, 4)}.

There is a wrinkle in this strategy, though, and that is that that type information about the closure is lost during serialization. The \textt{decode} function that we use to deserialize the \textt{Closure} after transmission requires that its type be known {\em a priori}. Unfortunately, the type of both the function and its environment is not available on the other side, and so we can't deserialize the closure. Even if we could, we couldn't call the function because we don't know the type of \textt{env}. This is because information about Haskell's types is not available at run-time. How, then, can we call a function with parameters, when we don't know their type?

The solution is to require the represented function itself to deserialize its own parameters. The same serialization mechanism that converts the Closure as a whole into a transmission-ready form can be applied to the function's tupled parameters. Haskell conveniently provides \textt{encode} and \textt{decode} functions in the \textt{Data.Binary} module which are able to convert any \textt{Serializable} data into a \textt{ByteString} and vice versa. So, we propose a revised version of our \textt{Closure} type:

\begin{code}
-- correct, but impractical
data Closure a = Closure (Static (ByteString -> a)) ByteString
\end{code}

We have replaced the tuple \textt{rep} in the earlier version with a concrete type, \textt{ByteString}, that represents an encoding of the function parameters. The first job of the function, then, will be to call \textt{decode} to get their values. The function can do this, while the framework cannot, because the framework doesn't know what type to deserialize the \textt{ByteString} into. It is ultimately only the function itself that knows the type of environment that it expects, and this information cannot be extracted at runtime.

This procedure of encoding and decoding function arguments is safe, even if by some accident the function tries to decode its arguments into the wrong type. In that case, \textt{decode} will throw an exception that is caught by the framework and passed back to the initiator.

If we assume that the language provides a serializable representation of all \textt{Static} types (that is, \textt{Static a} is an instance of \textt{Serializble}), our new definition of \textt{Closure} is sufficient to implement \textt{spawn} and \textt{call}, which now have the following types:

\begin{code}
-- correct!
spawn :: NodeId -> Closure (ProcessM ()) -> ProcessM ProcessId
call :: (Serializable a) => NodeId -> Closure a -> ProcessM a
\end{code}

Unfortunately, our consideration of the \textt{Static} type and its dependence on a notion of staticability requires extensions to the underlying language, and Haskell does not currently provide these extensions. How, then, to proceed?

\subsection{Closures in practice}

Our definition in the previous section allows a serializable representation of any staticable function, where a staticable function is a top-level function or a function whose free variables are staticable. A reasonable approximation of the set of staticable functions is the set of top-level functions, and as it turns out, providing serializable representations of any top-level function is a much easier goal than providing serializable representations of any staticable function; in fact, with this minor restriction, we are able to implement this feature, and the closures that depend on it, without extending the language. Furthermore, we feel that limiting serializability to top-level functions isn't a major additional restriction, since any staticable function can be trivially made top-level. Finally, top-level functions are all automatically staticable and cannot capture non-staticable names from their environment, and so are guaranteed to be safe to serialize.

The difficult part of function representation is that whatever identifier we use for a particular function must be unique and recognizable on both sides of the communication. Function pointers, while useful for representing functions on a single system, aren't a valid option in a distributed environment, since the remote system might have a different pointer width, and even if it doesn't variations in compiler and operating system render function pointers incomparable between computers. If we could select a unique identifier for each serializable function and transmit that identifier in place of the function itself, the receiving end would only need to map the identifier back to the original function, and we will have achieved our goal of transmitting a function representation. Such an identifier could take the place of the \textt{Static} type in \textt{Closure}s.

Fortunately, since we've restricted the set of serializable functions to top-level functions, we already have a convenient, globally unique identifier for each such function: its fully-qualified name. Here, a fully-qualified name consists of the module in which the function is defined and the name of the function, separated by a period. We can now consider yet another version of the \textt{Closure} data structure:

\begin{code}
-- correct!
data Closure a = Closure String ByteString
\end{code}

We've replaced the theoretical \textt{Static} type wrapper with a string, which stores the fully-qualified name of the represented function. Its parameters remain encoded as a \textt{ByteString}. Note that the function named by the string must have type \textt{ByteString -> a}. Before we can make this version of \textt{Closure} work, though, we need a way of mapping functions to their names, and those names back to the original function. This is not trivial, since Haskell does not provide run-time name lookup services.

We must provide a name lookup service ourselves. 



What can happen if something goes wrong? What, for example, will happen if we have different code on the two sides of communication? If the function named in the closure doesn't exist on the other side, looking it up in the function table will fail, and \textt{spawn} can report the error to the programmer. More insidiously, what if a function of the same name exists, but with a different environment? In this case, depending on how the types differs, it's possible for the environment's deserialization to not fail, but to succeed by extracting incorrect values from the \textt{ByteString}, which is worse than failing. We can eliminate this risk by including a representation of the environment type in the closure, and checking this type against the expected type on the remote end. In our implementation, we package the \textt{ByteString} of the function's environment along with a string representation of the environment's type.


\subsection{Closures, with sugar}

The above procedure for remotely invoking a closure seems prohibitively cumbersome. Fortunately, the Template Haskell facility lets us generate some sugar for all this which simplifies the procedure greatly. Template Haskell provides compile-time rewriting facilities that let us remove much of the tedium of writing wrapper functions. Our framework includes a compile-time \textt{remotable} function that operates on groups of Haskell function names and automatically produces wrapper functions and closure-generators that can be used with \textt{spawn}. Let's revisit the \textt{add} example from above. The programmer can request generation of the requisite stub functions using this syntax:

\begin{code}
$( remotable ['add] )
\end{code}

Here, the special brackets \textt{\$( )} demarcate code to be executed at compile time. The \textt{remotable} function is given a list of function names, each quoted with a single apostrophe to prevent its evaluation. The above \textt{remotable} call will produce the following code:

\begin{code}
-- the deserializing wrapper
add__impl :: ByteString -> Int
add__impl bs = let (i1, i2) = decode bs
                in add i1 i2

-- the closure generator
add__closure :: Int -> Int -> Closure Int
add__closure i2 i2 = let bs = encode (i1, i2)
                      in Closure "Main.add__impl" bs

-- the lookup table
__remoteCallMetaData = putReg "Main.add__impl" add__impl
\end{code}

Now, when the programmer wants to remotely invoke \textt{add}, all that's necessary is to use \textt{add\_\_closure} in place of \textt{add}:

\begin{code}
spawn aNode (add__closure 5 12)
\end{code}

Notice that this syntax isn't very far from blah blah

The underlying machinery takes care of the rest. \textt{add\_\_closure} serializes its arguments and returns a closure pointing to the closed function \textt{add\_\_impl}. When the closure the received by the remote host, the code corresponding the string \textt{"add\_\_impl"} can be looked up in \textt{\_\_remoteCallMetaData}, and \textt{add\_\_impl} will be called, given the serialized environment as a parameter. Because \textt{add\_\_impl} was generated bespoke for \textt{add}, it knows how to deserialize the environment and call the right implementation.

\begin{itemize}
\item local processes (with capture) vs. remote processes (without)
\item Serialization of closures
\item serialization doesn't work with existentials and gadts, polymorphic
\item theoretical ``hash arrows'' and their implementation as strings
\item convenient syntax thanks to Template Haskell
\item Combined process invocation and environment demarshalling
\item gettings results back from other sides
\end{itemize}


\section{Implementation}
Erlang has a nice feature that allows program modules to be updated over the wire. So, when a new version of code is released, it can be transmitted to every host in the network, where it will replace the old version of the code, without even having to restart the application. We decided not to go in this direction with our framework, partly because code update is a problem that can be separated from the other aspects of building a distributed computing framework, and partly because solving it is hard. The hardness is especially prohibitive in Haskell's case, which compiles programs to machine code and lets the operating system load them, where Erlang's bytecode interpreter retains more control over the loading and execution of programs.

A disadvantage of missing the dynamic updating is that code needs to be distributed to remote hosts out of band. In our development environment this was usually done with \textt{scp} and similar tools. Furthermore, this imposes the responsibility on the programmer to ensure that all hosts are running the same version of the compiled executable. Because we don't make any framework-level provision for rectifying incompatible message types, sending messages between executables that share message types with different structure would most likely crash the deserializer.

\begin{itemize}
\item based on Haskell's lightweight threads  and stm
\item Examples of configuration, deployment
\item syntax examples, comparison with Erlang
\item examples of applications: pi, k-means
\end{itemize}


\section{Examples}
Here, we show how this framework can be used to implement some well-known distributed algorithms.
pi calculator, omit sequence generator
fgrep


An example of storing process-local state using tail recursion.
\begin{code}
module Main where

data CounterMessage = CounterQuery ProcessId
                    | CounterIncrement
-- omitted: Serializable instance for CounterMessage

counterLoop :: Int -> ProcessM ()
counterLoop value =
  let
    counterComand (CounterQuery pid) =
      do { send pid value
         ; return value }
    counterComand CounterIncrement =
      return (value+1)
  in receiveWait [match counterCommand]
       >>= counterLoop

$( remotable ['counterLoop] )

increment :: ProcessId -> ProcessM ()
increment counterpid = send counterpid msg
  where msg = CounterIncrement

query :: ProcessId -> ProcessM Int
query counterpid =
  do { mypid <- getSelfPid
     ; let msg = CounterQuery mypid
     ; send counterpid msg
     ; receiveWait [match return] }

go _ =
  do { mynode <- getSelfNode
     ; counterpid <- spawn mynode (counterLoop__closure 0)
     ; increment counterpid
     ; increment counterpid
     ; newVal <- query counterpid
     ; print newVal }
     -- prints out 2

main = remoteInit (Just "config") [Main.__remoteCallMetaData] go
\end{code}

more example? node discovery? roles?

\section{Performance}
\begin{center}
\begin{table}[h]
 \caption{Performance of k-means clustering}
\begin{tabular}{l c c|r}
\hline
  \# points & \# reducers & \# mappers & Time (s) \\
  170,000 & 2 & 17 & 204 \\
  370,000 & 2 & 37 & 248 \\
  560,000 & 3 & 56 & 289 \\
  750,000 & 4 & 75 & 353 \\
\hline
\end{tabular}
\end{table}
\end{center}
\begin{itemize}
\item how to store process-local state
Haskell explicit about effects
message queues force state updates to be serialized as parameters to function using tail-recursion 

\item of k-means
\item what other interesting performance measures? graph?
\item with various numbers of hosts
\end{itemize}

\section{Related work}
\begin{itemize}
\item Erlang 
\item mpi 
\item Clean, dynamic types
\item RPC, stub generation: soap, IDL
\item Concurrent Haskell \cite{Parallel2008}, Glasgow Distributed Haskell \cite{gdh2001}
\item Dryad \cite{Dryad2007}, MapReduce \cite{MapReduce2008}, Skywriting \cite{Murray2010}
\end{itemize}

\section{Future work}
\begin{itemize}
\item Skywriting-y layer on top?
\item Isis/Paxos/virtual synchrony consensus?
\end{itemize}

Our framework is composed of two layers:

\begin{enumerate}
\item The task layer --- The basic unit of control here is the {\em task}, an idempotent, restartable block of code which produces a well-defined result. The framework takes care of allocating tasks to physical resources, resolving data dependencies between graphs, transmitting intermediary results, and recovering from failure. Data dependencies and tasks are represented as the edges and vertices, respectively, of a directed acyclic graph. Data dependencies are exposed to the programmer as {\em promises}.
\item The process layer --- This layer provides {\em processes}, units of concurrently executing code within a cloud. The framework provides facilities for starting processes on remote nodes, exchanging messages between them, and detecting their failure. The process layer provides the basis for the implementation of the task layer.
\end{enumerate}

Both of these layers are accessible as a small domain-specific language (DSL), embedded in Haskell as a monad. In this paper, we're focusing on the process layer.

% \appendix
% \section{Appendix Title}
% This is the text of the appendix, if you need one.

\acks

Alan Mycroft.

We thank John Launchbury who helped us see that $(\text{\tt Static}\;\tau)$
would be a better type for than a static \emph{function} type 
$\sigma \stackrel{\to}{\#} \tau$.

% The bibliography should be embedded for final submission.

\bibliographystyle{abbrvnat}
\bibliography{bib}

%\bibliographystyle{abbrvnat}
%\begin{thebibliography}{}
%\softraggedright
%
%\bibitem[Smith et~al.(2009)Smith, Jones]{smith02}
%P. Q. Smith, and X. Y. Jones. ...reference text...
%
%\end{thebibliography}

\end{document}
